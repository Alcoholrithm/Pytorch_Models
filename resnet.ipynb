{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "import copy\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityShortcut(nn.Module):\n",
    "    \"\"\"\n",
    "        A class to make Identity Mapping Shortcut\n",
    "\n",
    "        Attributes:\n",
    "            pooling : A max pooling layer \n",
    "            extra_cahnnel : The difference between input an output channels\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        \"\"\"\n",
    "            Initialize the Identity Shortcut Class\n",
    "\n",
    "            Args:\n",
    "                in_channels: number of input channels\n",
    "                out_channels: number of output channels\n",
    "                stride: size of stride\n",
    "\n",
    "            Returns:\n",
    "                None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pooling = nn.MaxPool2d(1, stride=stride)\n",
    "        self.extra_channel = out_channels - in_channels\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.pad(x, (0, 0, 0, 0, 0, self.extra_channel))\n",
    "        x = self.pooling(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        A class about residual block\n",
    "\n",
    "        When stride == 1, just add input value to output of residual block\n",
    "        When stride == 2, process the input value according to shortcut type and add it to output of residual block\n",
    "        Attributes:\n",
    "            residual_blcok: A sequential container to sequentially configure the layers for residual learning\n",
    "            shortcut: A shortcut connection of residual block\n",
    "            relu: A relu activation layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, shortcut_type = None):\n",
    "        \"\"\"\n",
    "            Initialize the residual block\n",
    "\n",
    "            Args:\n",
    "                in_channels: The number of input channels\n",
    "                out_channels: The number of output channels\n",
    "                kernel_size: The number of kernnel size in the convolution layers\n",
    "                stride: The size of stride\n",
    "                shortcut_type: The type of shortcut connection when stride is not 1\n",
    "            Returns:\n",
    "                None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.residual_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size, padding = 1, bias = False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        if stride != 1:\n",
    "            if shortcut_type == 'A':\n",
    "                self.shortcut = IdentityShortcut(in_channels, out_channels, stride)\n",
    "            elif shortcut_type == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels, 1, stride),\n",
    "                    nn.BatchNorm2d(out_channels)\n",
    "                )\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.residual_block(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "        A class about residual network\n",
    "\n",
    "        Attributes:\n",
    "            conv1_x: First set of layers\n",
    "            conv2_x: Second set of residual blocks\n",
    "            conv3_x: Third set of residual blocks\n",
    "            conv4_x: Fourth set of residual blocks\n",
    "            avg_pool: A average pooling layer of residual network\n",
    "            flatten: A flatten layer for the fully connected layer\n",
    "            fc: A fully connected layer to get probability of each class about given image\n",
    "    \"\"\"\n",
    "    def __init__(self, conv_num, num_classes):\n",
    "        \"\"\"\n",
    "            Initialize the residual network\n",
    "\n",
    "            Args:\n",
    "                conv_num: The number of residual blocks in each set of residual block\n",
    "                num_classes: The number of classes to predict\n",
    "            Returns:\n",
    "                None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1_x = nn.Sequential(\n",
    "                                    nn.Conv2d(3, 16, kernel_size = 3, stride = 1, padding=1, bias=False),\n",
    "                                    nn.BatchNorm2d(16),\n",
    "                                    nn.ReLU(),\n",
    "                                    #nn.MaxPool2d(kernel_size = 3, stride = 1, padding = 1)\n",
    "        )\n",
    "        \n",
    "        self.conv2_x = nn.Sequential(*[ResidualBlock(16, 16, 3, 1) for _ in range(conv_num[0])])\n",
    "        self.conv3_x = nn.Sequential(ResidualBlock(16, 32, 3, 2, 'A'), *[ResidualBlock(32, 32, 3, 1) for _ in range(conv_num[1] - 1)])\n",
    "        self.conv4_x = nn.Sequential(ResidualBlock(32, 64, 3, 2, 'A'), *[ResidualBlock(64, 64, 3, 1) for _ in range(conv_num[2] - 1)])\n",
    "        #self.conv5_x = nn.Sequential(ResidualBlock(256, 128, 3, 2), *[ResidualBlock(512, 512, 3, 1) for _ in range(conv_num[3])])\n",
    "\n",
    "        #self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.avg_pool = nn.AvgPool2d(8, stride=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(in_features=64,out_features=num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity = 'relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1_x(x)\n",
    "        x = self.conv2_x(x)\n",
    "        x = self.conv3_x(x)\n",
    "        x = self.conv4_x(x)\n",
    "        #x = self.conv5_x(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = ResNet([3, 3, 3], 10).to('cuda') # resnet20\n",
    "model = ResNet([5, 5, 5], 10).to('cuda') # resnet32\n",
    "#resnet_44 = ResNet([7, 7, 7], 10) # resnet44\n",
    "#resnet_56 = ResNet([9, 9, 9], 10) # resnet56\n",
    "#resnet_110 = ResNet([18, 18, 18], 10) # resnet110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 32, 32]             432\n",
      "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
      "              ReLU-3           [-1, 16, 32, 32]               0\n",
      "            Conv2d-4           [-1, 16, 32, 32]           2,304\n",
      "       BatchNorm2d-5           [-1, 16, 32, 32]              32\n",
      "              ReLU-6           [-1, 16, 32, 32]               0\n",
      "            Conv2d-7           [-1, 16, 32, 32]           2,304\n",
      "       BatchNorm2d-8           [-1, 16, 32, 32]              32\n",
      "              ReLU-9           [-1, 16, 32, 32]               0\n",
      "    ResidualBlock-10           [-1, 16, 32, 32]               0\n",
      "           Conv2d-11           [-1, 16, 32, 32]           2,304\n",
      "      BatchNorm2d-12           [-1, 16, 32, 32]              32\n",
      "             ReLU-13           [-1, 16, 32, 32]               0\n",
      "           Conv2d-14           [-1, 16, 32, 32]           2,304\n",
      "      BatchNorm2d-15           [-1, 16, 32, 32]              32\n",
      "             ReLU-16           [-1, 16, 32, 32]               0\n",
      "    ResidualBlock-17           [-1, 16, 32, 32]               0\n",
      "           Conv2d-18           [-1, 16, 32, 32]           2,304\n",
      "      BatchNorm2d-19           [-1, 16, 32, 32]              32\n",
      "             ReLU-20           [-1, 16, 32, 32]               0\n",
      "           Conv2d-21           [-1, 16, 32, 32]           2,304\n",
      "      BatchNorm2d-22           [-1, 16, 32, 32]              32\n",
      "             ReLU-23           [-1, 16, 32, 32]               0\n",
      "    ResidualBlock-24           [-1, 16, 32, 32]               0\n",
      "           Conv2d-25           [-1, 16, 32, 32]           2,304\n",
      "      BatchNorm2d-26           [-1, 16, 32, 32]              32\n",
      "             ReLU-27           [-1, 16, 32, 32]               0\n",
      "           Conv2d-28           [-1, 16, 32, 32]           2,304\n",
      "      BatchNorm2d-29           [-1, 16, 32, 32]              32\n",
      "             ReLU-30           [-1, 16, 32, 32]               0\n",
      "    ResidualBlock-31           [-1, 16, 32, 32]               0\n",
      "           Conv2d-32           [-1, 16, 32, 32]           2,304\n",
      "      BatchNorm2d-33           [-1, 16, 32, 32]              32\n",
      "             ReLU-34           [-1, 16, 32, 32]               0\n",
      "           Conv2d-35           [-1, 16, 32, 32]           2,304\n",
      "      BatchNorm2d-36           [-1, 16, 32, 32]              32\n",
      "             ReLU-37           [-1, 16, 32, 32]               0\n",
      "    ResidualBlock-38           [-1, 16, 32, 32]               0\n",
      "           Conv2d-39           [-1, 32, 16, 16]           4,608\n",
      "      BatchNorm2d-40           [-1, 32, 16, 16]              64\n",
      "             ReLU-41           [-1, 32, 16, 16]               0\n",
      "           Conv2d-42           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-43           [-1, 32, 16, 16]              64\n",
      "        MaxPool2d-44           [-1, 32, 16, 16]               0\n",
      " IdentityShortcut-45           [-1, 32, 16, 16]               0\n",
      "             ReLU-46           [-1, 32, 16, 16]               0\n",
      "    ResidualBlock-47           [-1, 32, 16, 16]               0\n",
      "           Conv2d-48           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-49           [-1, 32, 16, 16]              64\n",
      "             ReLU-50           [-1, 32, 16, 16]               0\n",
      "           Conv2d-51           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-52           [-1, 32, 16, 16]              64\n",
      "             ReLU-53           [-1, 32, 16, 16]               0\n",
      "    ResidualBlock-54           [-1, 32, 16, 16]               0\n",
      "           Conv2d-55           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-56           [-1, 32, 16, 16]              64\n",
      "             ReLU-57           [-1, 32, 16, 16]               0\n",
      "           Conv2d-58           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-59           [-1, 32, 16, 16]              64\n",
      "             ReLU-60           [-1, 32, 16, 16]               0\n",
      "    ResidualBlock-61           [-1, 32, 16, 16]               0\n",
      "           Conv2d-62           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-63           [-1, 32, 16, 16]              64\n",
      "             ReLU-64           [-1, 32, 16, 16]               0\n",
      "           Conv2d-65           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-66           [-1, 32, 16, 16]              64\n",
      "             ReLU-67           [-1, 32, 16, 16]               0\n",
      "    ResidualBlock-68           [-1, 32, 16, 16]               0\n",
      "           Conv2d-69           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-70           [-1, 32, 16, 16]              64\n",
      "             ReLU-71           [-1, 32, 16, 16]               0\n",
      "           Conv2d-72           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-73           [-1, 32, 16, 16]              64\n",
      "             ReLU-74           [-1, 32, 16, 16]               0\n",
      "    ResidualBlock-75           [-1, 32, 16, 16]               0\n",
      "           Conv2d-76             [-1, 64, 8, 8]          18,432\n",
      "      BatchNorm2d-77             [-1, 64, 8, 8]             128\n",
      "             ReLU-78             [-1, 64, 8, 8]               0\n",
      "           Conv2d-79             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-80             [-1, 64, 8, 8]             128\n",
      "        MaxPool2d-81             [-1, 64, 8, 8]               0\n",
      " IdentityShortcut-82             [-1, 64, 8, 8]               0\n",
      "             ReLU-83             [-1, 64, 8, 8]               0\n",
      "    ResidualBlock-84             [-1, 64, 8, 8]               0\n",
      "           Conv2d-85             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-86             [-1, 64, 8, 8]             128\n",
      "             ReLU-87             [-1, 64, 8, 8]               0\n",
      "           Conv2d-88             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-89             [-1, 64, 8, 8]             128\n",
      "             ReLU-90             [-1, 64, 8, 8]               0\n",
      "    ResidualBlock-91             [-1, 64, 8, 8]               0\n",
      "           Conv2d-92             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-93             [-1, 64, 8, 8]             128\n",
      "             ReLU-94             [-1, 64, 8, 8]               0\n",
      "           Conv2d-95             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-96             [-1, 64, 8, 8]             128\n",
      "             ReLU-97             [-1, 64, 8, 8]               0\n",
      "    ResidualBlock-98             [-1, 64, 8, 8]               0\n",
      "           Conv2d-99             [-1, 64, 8, 8]          36,864\n",
      "     BatchNorm2d-100             [-1, 64, 8, 8]             128\n",
      "            ReLU-101             [-1, 64, 8, 8]               0\n",
      "          Conv2d-102             [-1, 64, 8, 8]          36,864\n",
      "     BatchNorm2d-103             [-1, 64, 8, 8]             128\n",
      "            ReLU-104             [-1, 64, 8, 8]               0\n",
      "   ResidualBlock-105             [-1, 64, 8, 8]               0\n",
      "          Conv2d-106             [-1, 64, 8, 8]          36,864\n",
      "     BatchNorm2d-107             [-1, 64, 8, 8]             128\n",
      "            ReLU-108             [-1, 64, 8, 8]               0\n",
      "          Conv2d-109             [-1, 64, 8, 8]          36,864\n",
      "     BatchNorm2d-110             [-1, 64, 8, 8]             128\n",
      "            ReLU-111             [-1, 64, 8, 8]               0\n",
      "   ResidualBlock-112             [-1, 64, 8, 8]               0\n",
      "       AvgPool2d-113             [-1, 64, 1, 1]               0\n",
      "         Flatten-114                   [-1, 64]               0\n",
      "          Linear-115                   [-1, 10]             650\n",
      "================================================================\n",
      "Total params: 464,154\n",
      "Trainable params: 464,154\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 8.22\n",
      "Params size (MB): 1.77\n",
      "Estimated Total Size (MB): 10.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_cifar10 = CIFAR10(root = 'datasets/cifar10', train=True, download=True)\n",
    "val_cifar10 = CIFAR10(root = 'datasets/cifar10', train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = train_cifar10.data.mean(axis=(0,1,2)) / 255\n",
    "train_std = train_cifar10.data.std(axis=(0,1,2)) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "                                        #transforms.Resize(224),\n",
    "                                        transforms.RandomCrop(32, padding=4),\n",
    "                                        transforms.RandomHorizontalFlip(),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(train_mean, train_std),                                      \n",
    "                                        ])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "                                        transforms.ToTensor(),\n",
    "                                        #transforms.Resize(224),\n",
    "                                        transforms.Normalize(train_mean, train_std)\n",
    "                                        ])\n",
    "\n",
    "train_cifar10.transform = train_transforms\n",
    "val_cifar10.transform = train_transforms\n",
    "\n",
    "train_dl = DataLoader(train_cifar10, batch_size=256, shuffle=True, num_workers=4)\n",
    "val_dl = DataLoader(val_cifar10, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_correct(output, target):\n",
    "    pred = output.argmax(1, keepdim=True)\n",
    "    corrects = pred.eq(target.view_as(pred)).sum().item()\n",
    "    return corrects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [02:59<26:45, 17.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | train_loss = 0.49 |  train_acc = 83.18 | val_loss = 0.65 | val_acc = 78.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [05:58<23:54, 17.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | train_loss = 0.44 |  train_acc = 84.63 | val_loss = 0.52 | val_acc = 82.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [08:57<20:48, 17.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 | train_loss = 0.45 |  train_acc = 84.43 | val_loss = 0.51 | val_acc = 82.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [11:57<17:53, 17.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 | train_loss = 0.45 |  train_acc = 84.51 | val_loss = 0.52 | val_acc = 82.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [14:56<14:53, 17.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 | train_loss = 0.44 |  train_acc = 84.60 | val_loss = 0.51 | val_acc = 82.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [16:26<13:28, 17.97s/it]"
     ]
    }
   ],
   "source": [
    "#writer = SummaryWriter('resnet_logs')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1,\n",
    "                      momentum=0.9, weight_decay=1e-4)\n",
    "#optimizer = torch.optim.Adam(model.parameters())\n",
    "patience = 0\n",
    "device = 'cuda'\n",
    "model.to(device)\n",
    "epochs = 100\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "decay_epoch = [32000, 48000]\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=decay_epoch, gamma=0.1)\n",
    "#lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor = 0.1, patience=10)\n",
    "#accumulation_steps = 2\n",
    "best_model = None\n",
    "best_accs = -1\n",
    "for _ in tqdm(range(epochs)):\n",
    "    global_loss = 0\n",
    "    corrects = 0\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_dl):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "        loss = loss_func(output, target)\n",
    "        global_loss = global_loss + loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #if (batch_idx + 1) % accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        corrects += count_correct(output, target)\n",
    "\n",
    "    train_losses.append(global_loss / (batch_idx + 1))\n",
    "    train_accs.append(corrects / len(train_cifar10) * 100)\n",
    "    \n",
    "    model.eval()\n",
    "    corrects = 0\n",
    "    global_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(val_dl):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(data)\n",
    "            loss = loss_func(output, target)\n",
    "            global_loss = global_loss + loss.item()\n",
    "            corrects += count_correct(output, target)\n",
    "\n",
    "    val_losses.append(global_loss / (batch_idx + 1))\n",
    "    val_accs.append(corrects / len(val_cifar10) * 100)\n",
    "    \n",
    "    \n",
    "    if best_accs > val_accs[-1]:\n",
    "        patience = patience + 1\n",
    "        if patience == 3:\n",
    "            val_accs[-1] = best_accs\n",
    "            model = copy.deepcopy(best_model)\n",
    "            patience = 0\n",
    "    else:\n",
    "        best_accs = val_accs[-1]\n",
    "        best_model = copy.deepcopy(model)\n",
    "    \n",
    "    \n",
    "    #writer.add_scalar('resnet_log/train_error', 100 - train_accs[-1], _ + 1)\n",
    "    #writer.add_scalar('resnet_log/validation_error', 100 - val_accs[-1], _ + 1)\n",
    "    \n",
    "    if (_ + 1) % 10 == 0:\n",
    "        print(\"Epoch %d | train_loss = %.2f |  train_acc = %.2f | val_loss = %.2f | val_acc = %.2f\" % (_ + 1, train_losses[-1], train_accs[-1], val_losses[-1], val_accs[-1]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e229920edd51aea19e88d6f5beb6077fb3a4e96681a811b0949e41c8aa8fa312"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('oscar': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
